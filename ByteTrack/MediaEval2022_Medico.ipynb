{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8B2YrlmtDtiQ",
        "XvvUAUgDD7dt",
        "TBD6Ra8e6rHX",
        "vQcqTOUD6w5v",
        "8J7sqGuv01DB",
        "ZUeo2Snp6gaJ",
        "FJccOs2c2Voq",
        "brVqwdEIj-IG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisDo2108/MediaEval2022_Medico/blob/main/ByteTrack/MediaEval2022_Medico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rlomdIeKTwyC",
        "outputId": "75b3ecbf-6474-4017-8b5b-04866aca62d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Install libraries"
      ],
      "metadata": {
        "id": "QnYaTTTDDiDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MediaEval2022_Medico/\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "%cd ByteTrack\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 setup.py develop\n",
        "%cd /content/drive/MyDrive/MediaEval2022_Medico\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt  # install"
      ],
      "metadata": {
        "id": "xuAqL-XjsEd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip3 install cython_bbox\n",
        "!pip3 install natsort\n",
        "# !pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111"
      ],
      "metadata": {
        "id": "TE_AuWzYsLHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import cv2\n",
        "from natsort import natsorted\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "import torch"
      ],
      "metadata": {
        "id": "LplSDWtdsv-t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get VISEM dataset"
      ],
      "metadata": {
        "id": "dXu7fji7DpLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle"
      ],
      "metadata": {
        "id": "ikYCA_Ew6dXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()         # expire any previous token(s) and upload recreated token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Ry8srzvT6OBH",
        "outputId": "d381cdf8-bedb-4af8-cfd3-376796816a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-47b29845-86d0-466a-b309-b0d326ae862d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-47b29845-86d0-466a-b309-b0d326ae862d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tuanluchuynh\",\"key\":\"21fcfbce1597c00123a8fcf44f6257cc\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir /root/.kaggle/\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "nfLdiBuVwCGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MediaEval2022_Medico/\n",
        "!kaggle datasets download -d vlbthambawita/visemtracking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YSUOPf76GW4",
        "outputId": "ec8057e7-e6da-4c02-f040-fed731f634be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MediaEval2022_Medico\n",
            "Downloading visemtracking.zip to /content/drive/MyDrive/MediaEval2022_Medico\n",
            "100% 1.77G/1.78G [00:37<00:00, 78.0MB/s]\n",
            "100% 1.78G/1.78G [00:37<00:00, 50.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/MediaEval2022_Medico/visemtracking.zip -d /content/drive/MyDrive/MediaEval2022_Medico/"
      ],
      "metadata": {
        "id": "mzUGecFJvhSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare train-val COCO format dataset"
      ],
      "metadata": {
        "id": "8B2YrlmtDtiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Train'\n",
        "num = np.sort(np.array(os.listdir(IMAGE_PATH), dtype='int32'))\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_num, val_num = train_test_split(num, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FvTncNKm7Czw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CREATE A VAL FOLDER AND MOVE\n",
        "Path('/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/').mkdir(parents=True, exist_ok=True)\n",
        "Path('/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/annotations/').mkdir(parents=True, exist_ok=True)\n",
        "for folder in val_num:\n",
        "    source = os.path.join(IMAGE_PATH, str(folder))\n",
        "    dest = \"/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/\"\n",
        "    !mv $source $dest"
      ],
      "metadata": {
        "id": "S83euuoC7c_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same script for MOT17\n",
        "DATA_PATH = '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/'\n",
        "OUT_PATH = os.path.join(DATA_PATH, 'annotations')\n",
        "SPLITS = ['Train', 'Val']\n",
        "\n",
        "def ccwh2xyxy(img_h, img_w, bb_box):\n",
        "    norm_center_x = bb_box[0]\n",
        "    norm_center_y = bb_box[1]\n",
        "    norm_label_width = bb_box[2]\n",
        "    norm_label_height = bb_box[3]\n",
        "    \n",
        "    center_x = norm_center_x * img_w\n",
        "    center_y = norm_center_y * img_h\n",
        "    label_width = norm_label_width * img_w\n",
        "    label_height = norm_label_height * img_h\n",
        "    \n",
        "    x_min = center_x - (label_width/2)\n",
        "    y_min = center_y - (label_height/2)\n",
        "    x_max = center_x + (label_width/2)\n",
        "    y_max = center_y + (label_height/2)\n",
        "    \n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "def xyxy2xywh(bboxes):\n",
        "    bboxes[2] = bboxes[2] - bboxes[0]\n",
        "    bboxes[3] = bboxes[3] - bboxes[1]\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    if not os.path.exists(OUT_PATH):\n",
        "        os.makedirs(OUT_PATH)\n",
        "\n",
        "    for split in SPLITS:\n",
        "        if split == \"Val\":\n",
        "            data_path = os.path.join(DATA_PATH, 'Val')\n",
        "        else:\n",
        "            data_path = os.path.join(DATA_PATH, 'Train')\n",
        "        out_path = os.path.join(OUT_PATH, '{}.json'.format(split))\n",
        "        out = {'images': [], 'annotations': [], 'videos': [],\n",
        "               'categories': [{'id': 0, 'name': 'sperm'},\n",
        "                              {'id': 1, 'name': 'cluster'},\n",
        "                              {'id': 2, 'name': 'small or pinhead'}]}\n",
        "        seqs = os.listdir(data_path)\n",
        "        image_cnt = 0\n",
        "        ann_cnt = 0\n",
        "        video_cnt = 0\n",
        "        tid_curr = 0\n",
        "        tid_last = -1\n",
        "\n",
        "        for seq in natsorted(seqs):\n",
        "            if '.DS_Store' in seq:\n",
        "                continue\n",
        "            if 'mot' in DATA_PATH and (split != 'test' and not ('FRCNN' in seq)):\n",
        "                continue\n",
        "            if seq.endswith('.ipynb_checkpoints'):\n",
        "                continue\n",
        "            video_cnt += 1  # video sequence number.\n",
        "            out['videos'].append({'id': video_cnt, 'file_name': seq})\n",
        "            seq_path = os.path.join(data_path, seq)\n",
        "            img_path = os.path.join(seq_path, 'images')\n",
        "            ann_path = os.path.join(seq_path, 'labels_ftid')\n",
        "            images = os.listdir(img_path)\n",
        "            # half and half\n",
        "            num_images = len([image for image in images if 'jpg' in image])\n",
        "            image_range = [0, num_images - 1]\n",
        "            print('{}: {} images'.format(seq, num_images))\n",
        "\n",
        "            for i, img_name in enumerate(natsorted(images)):\n",
        "                if i < image_range[0] or i > image_range[1]:\n",
        "                    continue\n",
        "                # Image\n",
        "                img = cv2.imread(os.path.join(\n",
        "                    img_path, img_name))\n",
        "                print('img path:', os.path.join(\n",
        "                    img_path, img_name))\n",
        "                height, width = img.shape[:2]\n",
        "                image_info = {'file_name': os.path.join(img_path, img_name),  # image name.\n",
        "                              # image number in the entire training set.\n",
        "                              'id': image_cnt + i + 1,\n",
        "                              # image number in the video sequence, starting from 1.\n",
        "                              'frame_id': i + 1 - image_range[0],\n",
        "                              # image number in the entire training set.\n",
        "                              'prev_image_id': image_cnt + i if i > 0 else -1,\n",
        "                              'next_image_id': image_cnt + i + 2 if i < num_images - 1 else -1,\n",
        "                              'video_id': seq, #video_cnt\n",
        "                              'height': height, 'width': width}\n",
        "                out['images'].append(image_info)\n",
        "                \n",
        "                # Label\n",
        "                annotations_path = os.path.join(\n",
        "                    ann_path, '{}_with_ftid.txt'.format(img_name[:-4]))\n",
        "                track_id_dict = {}\n",
        "                track_id_count = 1\n",
        "                if os.path.exists(annotations_path) == True:\n",
        "                    # anns = np.loadtxt(annotations_path,\n",
        "                    #                 dtype=np.float64, delimiter=' ')\n",
        "                    anns = np.genfromtxt(annotations_path, dtype='str')\n",
        "                    anns = np.atleast_2d(anns)\n",
        "                    print('label path:', annotations_path)\n",
        "                    for j in range(anns.shape[0]):\n",
        "                        frame_id = int(img_name[:-4].split('_')[-1])\n",
        "                        if frame_id - 1 < image_range[0] or frame_id - 1 > image_range[1]:\n",
        "                            continue\n",
        "                        track_id_str = anns[j][0]\n",
        "                        # Hashing track_id\n",
        "                        if track_id_str in track_id_dict.keys():\n",
        "                            track_id = track_id_dict[track_id_str]\n",
        "                        else:\n",
        "                            track_id_dict[track_id_str] = track_id_count\n",
        "                            track_id = track_id_count\n",
        "                            track_id_count += 1\n",
        "                        ###\n",
        "                        category_id = int(anns[j][1])\n",
        "                        bbox = np.array(anns[j][2:], dtype='float64').tolist()\n",
        "                        \n",
        "                        ### Convert yolo bbox to coco format ###\n",
        "                        bbox = ccwh2xyxy(height, width, bbox)\n",
        "                        bbox = xyxy2xywh(bbox)\n",
        "                        # print(bbox)\n",
        "                        ### Convert yolo bbox to coco format ###\n",
        "                        area = np.float64(anns[j][-1]) * \\\n",
        "                            np.float64(anns[j][-2])\n",
        "                        # frame_id = j + 1 - image_range[0]\n",
        "                        ann_cnt += 1\n",
        "                        if not track_id == tid_last:\n",
        "                            tid_curr += 1\n",
        "                            tid_last = track_id\n",
        "                        ann = {\n",
        "                            'id': ann_cnt,\n",
        "                            'category_id': category_id,\n",
        "                            'image_id': image_cnt + frame_id,\n",
        "                            'track_id': track_id,\n",
        "                            'bbox': bbox,\n",
        "                            'conf': 1,\n",
        "                            'iscrowd': 0,\n",
        "                            'area': area,\n",
        "                        }\n",
        "                        out['annotations'].append(ann)\n",
        "            image_cnt += num_images\n",
        "            print(tid_curr, tid_last)\n",
        "            # break\n",
        "        print('loaded {} for {} images and {} annotations'.format(\n",
        "            split, len(out['images']), len(out['annotations'])))\n",
        "        json.dump(out, open(out_path, 'w'))"
      ],
      "metadata": {
        "id": "E5srUKF9cLGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config ByteTrack"
      ],
      "metadata": {
        "id": "XvvUAUgDD7dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/exps/example/mot/yolox_x_ch.py' \\\n",
        "    '/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/exps/example/mot/visem.py'"
      ],
      "metadata": {
        "id": "EliHTF7_BBfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /content/ByteTrack/exps/example/mot/visem.py"
      ],
      "metadata": {
        "id": "TBD6Ra8e6rHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# encoding: utf-8\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "\n",
        "from yolox.exp import Exp as MyExp\n",
        "from yolox.data import get_yolox_datadir\n",
        "\n",
        "class Exp(MyExp):\n",
        "    def __init__(self):\n",
        "        super(Exp, self).__init__()\n",
        "        self.num_classes = 3\n",
        "        self.depth = 1.33\n",
        "        self.width = 1.25\n",
        "        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n",
        "        self.train_ann = \"/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/annotations/Train.json\"\n",
        "        self.val_ann = \"/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/annotations/Val.json\"\n",
        "        self.input_size = (480, 640)\n",
        "        self.test_size = (480, 640)\n",
        "        self.random_size = (18, 32)\n",
        "        self.max_epoch = 10\n",
        "        self.print_interval = 20\n",
        "        self.eval_interval = 5\n",
        "        self.test_conf = 0.1\n",
        "        self.nmsthre = 0.7\n",
        "        self.no_aug_epochs = 10\n",
        "        self.basic_lr_per_img = 0.001 / 64.0\n",
        "        self.warmup_epochs = 1\n",
        "\n",
        "    def get_data_loader(self, batch_size, is_distributed, no_aug=False):\n",
        "        from yolox.data import (\n",
        "            MOTDataset,\n",
        "            TrainTransform,\n",
        "            YoloBatchSampler,\n",
        "            DataLoader,\n",
        "            InfiniteSampler,\n",
        "            MosaicDetection,\n",
        "        )\n",
        "\n",
        "        dataset = MOTDataset(\n",
        "            data_dir=os.path.join(get_yolox_datadir(), \"ch_all\"),\n",
        "            json_file=self.train_ann,\n",
        "            name='',\n",
        "            img_size=self.input_size,\n",
        "            preproc=TrainTransform(\n",
        "                rgb_means=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225),\n",
        "                max_labels=500,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        dataset = MosaicDetection(\n",
        "            dataset,\n",
        "            mosaic=not no_aug,\n",
        "            img_size=self.input_size,\n",
        "            preproc=TrainTransform(\n",
        "                rgb_means=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225),\n",
        "                max_labels=1000,\n",
        "            ),\n",
        "            degrees=self.degrees,\n",
        "            translate=self.translate,\n",
        "            scale=self.scale,\n",
        "            shear=self.shear,\n",
        "            perspective=self.perspective,\n",
        "            enable_mixup=self.enable_mixup,\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if is_distributed:\n",
        "            batch_size = batch_size // dist.get_world_size()\n",
        "\n",
        "        sampler = InfiniteSampler(\n",
        "            len(self.dataset), seed=self.seed if self.seed else 0\n",
        "        )\n",
        "\n",
        "        batch_sampler = YoloBatchSampler(\n",
        "            sampler=sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=False,\n",
        "            input_dimension=self.input_size,\n",
        "            mosaic=not no_aug,\n",
        "        )\n",
        "\n",
        "        dataloader_kwargs = {\"num_workers\": self.data_num_workers, \"pin_memory\": True}\n",
        "        dataloader_kwargs[\"batch_sampler\"] = batch_sampler\n",
        "        train_loader = DataLoader(self.dataset, **dataloader_kwargs)\n",
        "\n",
        "        return train_loader\n",
        "\n",
        "    def get_eval_loader(self, batch_size, is_distributed, testdev=False):\n",
        "        from yolox.data import MOTDataset, ValTransform\n",
        "\n",
        "        valdataset = MOTDataset(\n",
        "            data_dir=os.path.join(get_yolox_datadir(), \"mot\"),\n",
        "            json_file=self.val_ann,\n",
        "            img_size=self.test_size,\n",
        "            name='train',\n",
        "            preproc=ValTransform(\n",
        "                rgb_means=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if is_distributed:\n",
        "            batch_size = batch_size // dist.get_world_size()\n",
        "            sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                valdataset, shuffle=False\n",
        "            )\n",
        "        else:\n",
        "            sampler = torch.utils.data.SequentialSampler(valdataset)\n",
        "\n",
        "        dataloader_kwargs = {\n",
        "            \"num_workers\": self.data_num_workers,\n",
        "            \"pin_memory\": True,\n",
        "            \"sampler\": sampler,\n",
        "        }\n",
        "        dataloader_kwargs[\"batch_size\"] = batch_size\n",
        "        val_loader = torch.utils.data.DataLoader(valdataset, **dataloader_kwargs)\n",
        "\n",
        "        return val_loader\n",
        "\n",
        "    def get_evaluator(self, batch_size, is_distributed, testdev=False):\n",
        "        from yolox.evaluators import COCOEvaluator\n",
        "\n",
        "        val_loader = self.get_eval_loader(batch_size, is_distributed, testdev=testdev)\n",
        "        evaluator = COCOEvaluator(\n",
        "            dataloader=val_loader,\n",
        "            img_size=self.test_size,\n",
        "            confthre=self.test_conf,\n",
        "            nmsthre=self.nmsthre,\n",
        "            num_classes=self.num_classes,\n",
        "            testdev=testdev,\n",
        "        )\n",
        "        return evaluator\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "v2-0buwJ6kKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /content/ByteTrack/yolox/data/datasets/mot.py"
      ],
      "metadata": {
        "id": "vQcqTOUD6w5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# This is formatted as code\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import os\n",
        "\n",
        "from ..dataloading import get_yolox_datadir\n",
        "from .datasets_wrapper import Dataset\n",
        "\n",
        "\n",
        "class MOTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCO dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir=None,\n",
        "        json_file=\"train_half.json\",\n",
        "        name=\"train\",\n",
        "        img_size=(608, 1088),\n",
        "        preproc=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        COCO dataset initialization. Annotation data are read into memory by COCO API.\n",
        "        Args:\n",
        "            data_dir (str): dataset root directory\n",
        "            json_file (str): COCO json file name\n",
        "            name (str): COCO data name (e.g. 'train2017' or 'val2017')\n",
        "            img_size (int): target image size after pre-processing\n",
        "            preproc: data augmentation strategy\n",
        "        \"\"\"\n",
        "        super().__init__(img_size)\n",
        "        if data_dir is None:\n",
        "            data_dir = os.path.join(get_yolox_datadir(), \"mot\")\n",
        "        self.data_dir = data_dir\n",
        "        self.json_file = json_file\n",
        "\n",
        "        self.coco = COCO(os.path.join(self.data_dir, \"annotations\", self.json_file))\n",
        "        self.ids = self.coco.getImgIds()\n",
        "        self.class_ids = sorted(self.coco.getCatIds())\n",
        "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self._classes = tuple([c[\"name\"] for c in cats])\n",
        "        self.annotations = self._load_coco_annotations()\n",
        "        self.name = name\n",
        "        self.img_size = img_size\n",
        "        self.preproc = preproc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def _load_coco_annotations(self):\n",
        "        return [self.load_anno_from_ids(_ids) for _ids in self.ids]\n",
        "\n",
        "    def load_anno_from_ids(self, id_):\n",
        "        im_ann = self.coco.loadImgs(id_)[0]\n",
        "        width = im_ann[\"width\"]\n",
        "        height = im_ann[\"height\"]\n",
        "        frame_id = im_ann[\"frame_id\"]\n",
        "        video_id = im_ann[\"video_id\"]\n",
        "        anno_ids = self.coco.getAnnIds(imgIds=[int(id_)], iscrowd=False)\n",
        "        annotations = self.coco.loadAnns(anno_ids)\n",
        "        objs = []\n",
        "        for obj in annotations:\n",
        "            ### Modified\n",
        "            if len(obj['bbox']) == 0:\n",
        "                objs.append(obj)\n",
        "                obj[\"clean_bbox\"] = [0,0,0,0]\n",
        "                continue\n",
        "            ###\n",
        "            x1 = obj[\"bbox\"][0]\n",
        "            y1 = obj[\"bbox\"][1]\n",
        "            x2 = x1 + obj[\"bbox\"][2]\n",
        "            y2 = y1 + obj[\"bbox\"][3]\n",
        "            if obj[\"area\"] > 0 and x2 >= x1 and y2 >= y1:\n",
        "                obj[\"clean_bbox\"] = [x1, y1, x2, y2]\n",
        "                objs.append(obj)\n",
        "\n",
        "        num_objs = len(objs)\n",
        "\n",
        "        res = np.zeros((num_objs, 6))\n",
        "\n",
        "        for ix, obj in enumerate(objs):\n",
        "            cls = self.class_ids.index(obj[\"category_id\"])\n",
        "            res[ix, 0:4] = obj[\"clean_bbox\"]\n",
        "            res[ix, 4] = cls\n",
        "            res[ix, 5] = obj[\"track_id\"]\n",
        "\n",
        "        file_name = im_ann[\"file_name\"] if \"file_name\" in im_ann else \"{:012}\".format(id_) + \".jpg\"\n",
        "        img_info = (height, width, frame_id, video_id, file_name)\n",
        "\n",
        "        del im_ann, annotations\n",
        "\n",
        "        return (res, img_info, file_name)\n",
        "\n",
        "    def load_anno(self, index):\n",
        "        return self.annotations[index][0]\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        id_ = self.ids[index]\n",
        "\n",
        "        res, img_info, file_name = self.annotations[index]\n",
        "        # load image and preprocess\n",
        "        img_file = os.path.join(\n",
        "            self.data_dir, self.name, file_name\n",
        "        )\n",
        "        img = cv2.imread(img_file)\n",
        "        assert img is not None\n",
        "\n",
        "        return img, res.copy(), img_info, np.array([id_])\n",
        "\n",
        "    @Dataset.resize_getitem\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        One image / label pair for the given index is picked up and pre-processed.\n",
        "\n",
        "        Args:\n",
        "            index (int): data index\n",
        "\n",
        "        Returns:\n",
        "            img (numpy.ndarray): pre-processed image\n",
        "            padded_labels (torch.Tensor): pre-processed label data.\n",
        "                The shape is :math:`[max_labels, 5]`.\n",
        "                each label consists of [class, xc, yc, w, h]:\n",
        "                    class (float): class index.\n",
        "                    xc, yc (float) : center of bbox whose values range from 0 to 1.\n",
        "                    w, h (float) : size of bbox whose values range from 0 to 1.\n",
        "            info_img : tuple of h, w, nh, nw, dx, dy.\n",
        "                h, w (int): original shape of the image\n",
        "                nh, nw (int): shape of the resized image without padding\n",
        "                dx, dy (int): pad size\n",
        "            img_id (int): same as the input index. Used for evaluation.\n",
        "        \"\"\"\n",
        "        img, target, img_info, img_id = self.pull_item(index)\n",
        "\n",
        "        if self.preproc is not None:\n",
        "            img, target = self.preproc(img, target, self.input_dim)\n",
        "        return img, target, img_info, img_id\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "UYDrojQp6F-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/yolox/evaluators/mot_evaluator.py"
      ],
      "metadata": {
        "id": "8J7sqGuv01DB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from collections import defaultdict\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "from yolox.utils import (\n",
        "    gather,\n",
        "    is_main_process,\n",
        "    postprocess,\n",
        "    synchronize,\n",
        "    time_synchronized,\n",
        "    xyxy2xywh\n",
        ")\n",
        "from yolox.tracker.byte_tracker import BYTETracker\n",
        "from yolox.sort_tracker.sort import Sort\n",
        "from yolox.deepsort_tracker.deepsort import DeepSort\n",
        "from yolox.motdt_tracker.motdt_tracker import OnlineTracker\n",
        "\n",
        "import contextlib\n",
        "import io\n",
        "import os\n",
        "import itertools\n",
        "import json\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "\n",
        "def write_results(filename, results):\n",
        "    save_format = '{frame},{id},{x1},{y1},{w},{h},{s},-1,-1,-1\\n'\n",
        "    with open(filename, 'w') as f:\n",
        "        for frame_id, tlwhs, track_ids, scores in results:\n",
        "            for tlwh, track_id, score in zip(tlwhs, track_ids, scores):\n",
        "                if track_id < 0:\n",
        "                    continue\n",
        "                x1, y1, w, h = tlwh\n",
        "                line = save_format.format(frame=frame_id, id=track_id, x1=round(x1, 1), y1=round(y1, 1), w=round(w, 1), h=round(h, 1), s=round(score, 2))\n",
        "                f.write(line)\n",
        "    logger.info('save results to {}'.format(filename))\n",
        "\n",
        "\n",
        "def write_results_no_score(filename, results):\n",
        "    save_format = '{frame},{id},{x1},{y1},{w},{h},-1,-1,-1,-1\\n'\n",
        "    with open(filename, 'w') as f:\n",
        "        for frame_id, tlwhs, track_ids in results:\n",
        "            for tlwh, track_id in zip(tlwhs, track_ids):\n",
        "                if track_id < 0:\n",
        "                    continue\n",
        "                x1, y1, w, h = tlwh\n",
        "                line = save_format.format(frame=frame_id, id=track_id, x1=round(x1, 1), y1=round(y1, 1), w=round(w, 1), h=round(h, 1))\n",
        "                f.write(line)\n",
        "    logger.info('save results to {}'.format(filename))\n",
        "\n",
        "\n",
        "class MOTEvaluator:\n",
        "    \"\"\"\n",
        "    COCO AP Evaluation class.  All the data in the val2017 dataset are processed\n",
        "    and evaluated by COCO API.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, args, dataloader, img_size, confthre, nmsthre, num_classes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataloader (Dataloader): evaluate dataloader.\n",
        "            img_size (int): image size after preprocess. images are resized\n",
        "                to squares whose shape is (img_size, img_size).\n",
        "            confthre (float): confidence threshold ranging from 0 to 1, which\n",
        "                is defined in the config file.\n",
        "            nmsthre (float): IoU threshold of non-max supression ranging from 0 to 1.\n",
        "        \"\"\"\n",
        "        self.dataloader = dataloader\n",
        "        self.img_size = img_size\n",
        "        self.confthre = confthre\n",
        "        self.nmsthre = nmsthre\n",
        "        self.num_classes = num_classes\n",
        "        self.args = args\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        model,\n",
        "        distributed=False,\n",
        "        half=False,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        test_size=None,\n",
        "        result_folder=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        COCO average precision (AP) Evaluation. Iterate inference on the test dataset\n",
        "        and the results are evaluated by COCO API.\n",
        "\n",
        "        NOTE: This function will change training mode to False, please save states if needed.\n",
        "\n",
        "        Args:\n",
        "            model : model to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            ap50_95 (float) : COCO AP of IoU=50:95\n",
        "            ap50 (float) : COCO AP of IoU=50\n",
        "            summary (sr): summary info of evaluation.\n",
        "        \"\"\"\n",
        "        # TODO half to amp_test\n",
        "        tensor_type = torch.cuda.HalfTensor if half else torch.cuda.FloatTensor\n",
        "        model = model.eval()\n",
        "        if half:\n",
        "            model = model.half()\n",
        "        ids = []\n",
        "        data_list = []\n",
        "        results = []\n",
        "        video_names = defaultdict()\n",
        "        progress_bar = tqdm if is_main_process() else iter\n",
        "\n",
        "        inference_time = 0\n",
        "        track_time = 0\n",
        "        n_samples = len(self.dataloader) - 1\n",
        "\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones(1, 3, test_size[0], test_size[1]).cuda()\n",
        "            model(x)\n",
        "            model = model_trt\n",
        "            \n",
        "        tracker = BYTETracker(self.args)\n",
        "        ori_thresh = self.args.track_thresh\n",
        "        for cur_iter, (imgs, _, info_imgs, ids) in enumerate(\n",
        "            progress_bar(self.dataloader)\n",
        "        ):\n",
        "            with torch.no_grad():\n",
        "                # init tracker\n",
        "                frame_id = info_imgs[2].item()\n",
        "                video_id = info_imgs[3].item()\n",
        "                img_file_name = info_imgs[4]\n",
        "                video_name = img_file_name[0].split('/')[0]\n",
        "                if video_name == 'MOT17-05-FRCNN' or video_name == 'MOT17-06-FRCNN':\n",
        "                    self.args.track_buffer = 14\n",
        "                elif video_name == 'MOT17-13-FRCNN' or video_name == 'MOT17-14-FRCNN':\n",
        "                    self.args.track_buffer = 25\n",
        "                else:\n",
        "                    self.args.track_buffer = 30\n",
        "\n",
        "                if video_name == 'MOT17-01-FRCNN':\n",
        "                    self.args.track_thresh = 0.65\n",
        "                elif video_name == 'MOT17-06-FRCNN':\n",
        "                    self.args.track_thresh = 0.65\n",
        "                elif video_name == 'MOT17-12-FRCNN':\n",
        "                    self.args.track_thresh = 0.7\n",
        "                elif video_name == 'MOT17-14-FRCNN':\n",
        "                    self.args.track_thresh = 0.67\n",
        "                elif video_name in ['MOT20-06', 'MOT20-08']:\n",
        "                    self.args.track_thresh = 0.3\n",
        "                else:\n",
        "                    self.args.track_thresh = ori_thresh\n",
        "\n",
        "                if video_name not in video_names:\n",
        "                    video_names[video_id] = video_name\n",
        "                if frame_id == 1:\n",
        "                    tracker = BYTETracker(self.args)\n",
        "                    if len(results) != 0:\n",
        "                        result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id - 1]))\n",
        "                        write_results(result_filename, results)\n",
        "                        results = []\n",
        "\n",
        "                imgs = imgs.type(tensor_type)\n",
        "\n",
        "                # skip the the last iters since batchsize might be not enough for batch inference\n",
        "                is_time_record = cur_iter < len(self.dataloader) - 1\n",
        "                if is_time_record:\n",
        "                    start = time.time()\n",
        "\n",
        "                # outputs = model(imgs)\n",
        "                outputs = model(img_file_name)\n",
        "                # if decoder is not None:\n",
        "                #     outputs = decoder(outputs, dtype=outputs.type())\n",
        "\n",
        "                # outputs = postprocess(outputs, self.num_classes, self.confthre, self.nmsthre)\n",
        "            \n",
        "                if is_time_record:\n",
        "                    infer_end = time_synchronized()\n",
        "                    inference_time += infer_end - start\n",
        "\n",
        "            # output_results = self.convert_to_coco_format(outputs, info_imgs, ids)\n",
        "            # data_list.extend(output_results)\n",
        "\n",
        "            # run tracking\n",
        "            if outputs[0] is not None:\n",
        "                img_info = [info_imgs[0].item(), info_imgs[1].item()]\n",
        "                output_results = outputs.pred[0][:, :5].cpu().numpy()\n",
        "                online_targets = tracker.update(output_results, info_imgs, self.img_size)\n",
        "                online_tlwhs = []\n",
        "                online_ids = []\n",
        "                online_scores = []\n",
        "                for t in online_targets:\n",
        "                    tlwh = t.tlwh\n",
        "                    tid = t.track_id\n",
        "                    vertical = tlwh[2] / tlwh[3] > 1.6\n",
        "                    if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:\n",
        "                        online_tlwhs.append(tlwh)\n",
        "                        online_ids.append(tid)\n",
        "                        online_scores.append(t.score)\n",
        "                # save results\n",
        "                results.append((frame_id, online_tlwhs, online_ids, online_scores))\n",
        "\n",
        "            if is_time_record:\n",
        "                track_end = time_synchronized()\n",
        "                track_time += track_end - infer_end\n",
        "            \n",
        "            if cur_iter == len(self.dataloader) - 1:\n",
        "                result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id]))\n",
        "                write_results(result_filename, results)\n",
        "            break\n",
        "\n",
        "        statistics = torch.cuda.FloatTensor([inference_time, track_time, n_samples])\n",
        "        if distributed:\n",
        "            data_list = gather(data_list, dst=0)\n",
        "            data_list = list(itertools.chain(*data_list))\n",
        "            torch.distributed.reduce(statistics, dst=0)\n",
        "\n",
        "        eval_results = self.evaluate_prediction(data_list, statistics)\n",
        "        synchronize()\n",
        "        return eval_results\n",
        "\n",
        "    def evaluate_sort(\n",
        "        self,\n",
        "        model,\n",
        "        distributed=False,\n",
        "        half=False,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        test_size=None,\n",
        "        result_folder=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        COCO average precision (AP) Evaluation. Iterate inference on the test dataset\n",
        "        and the results are evaluated by COCO API.\n",
        "\n",
        "        NOTE: This function will change training mode to False, please save states if needed.\n",
        "\n",
        "        Args:\n",
        "            model : model to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            ap50_95 (float) : COCO AP of IoU=50:95\n",
        "            ap50 (float) : COCO AP of IoU=50\n",
        "            summary (sr): summary info of evaluation.\n",
        "        \"\"\"\n",
        "        # TODO half to amp_test\n",
        "        tensor_type = torch.cuda.HalfTensor if half else torch.cuda.FloatTensor\n",
        "        model = model.eval()\n",
        "        if half:\n",
        "            model = model.half()\n",
        "        ids = []\n",
        "        data_list = []\n",
        "        results = []\n",
        "        video_names = defaultdict()\n",
        "        progress_bar = tqdm if is_main_process() else iter\n",
        "\n",
        "        inference_time = 0\n",
        "        track_time = 0\n",
        "        n_samples = len(self.dataloader) - 1\n",
        "\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones(1, 3, test_size[0], test_size[1]).cuda()\n",
        "            model(x)\n",
        "            model = model_trt\n",
        "            \n",
        "        tracker = Sort(self.args.track_thresh)\n",
        "        \n",
        "        for cur_iter, (imgs, _, info_imgs, ids) in enumerate(\n",
        "            progress_bar(self.dataloader)\n",
        "        ):\n",
        "            with torch.no_grad():\n",
        "                # init tracker\n",
        "                frame_id = info_imgs[2].item()\n",
        "                video_id = info_imgs[3].item()\n",
        "                img_file_name = info_imgs[4]\n",
        "                video_name = img_file_name[0].split('/')[0]\n",
        "\n",
        "                if video_name not in video_names:\n",
        "                    video_names[video_id] = video_name\n",
        "                if frame_id == 1:\n",
        "                    tracker = Sort(self.args.track_thresh)\n",
        "                    if len(results) != 0:\n",
        "                        result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id - 1]))\n",
        "                        write_results_no_score(result_filename, results)\n",
        "                        results = []\n",
        "\n",
        "                imgs = imgs.type(tensor_type)\n",
        "\n",
        "                # skip the the last iters since batchsize might be not enough for batch inference\n",
        "                is_time_record = cur_iter < len(self.dataloader) - 1\n",
        "                if is_time_record:\n",
        "                    start = time.time()\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                if decoder is not None:\n",
        "                    outputs = decoder(outputs, dtype=outputs.type())\n",
        "\n",
        "                outputs = postprocess(outputs, self.num_classes, self.confthre, self.nmsthre)\n",
        "            \n",
        "                if is_time_record:\n",
        "                    infer_end = time_synchronized()\n",
        "                    inference_time += infer_end - start\n",
        "\n",
        "            output_results = self.convert_to_coco_format(outputs, info_imgs, ids)\n",
        "            data_list.extend(output_results)\n",
        "\n",
        "            # run tracking\n",
        "            online_targets = tracker.update(outputs[0], info_imgs, self.img_size)\n",
        "            online_tlwhs = []\n",
        "            online_ids = []\n",
        "            for t in online_targets:\n",
        "                tlwh = [t[0], t[1], t[2] - t[0], t[3] - t[1]]\n",
        "                tid = t[4]\n",
        "                vertical = tlwh[2] / tlwh[3] > 1.6\n",
        "                if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:\n",
        "                    online_tlwhs.append(tlwh)\n",
        "                    online_ids.append(tid)\n",
        "            # save results\n",
        "            results.append((frame_id, online_tlwhs, online_ids))\n",
        "\n",
        "            if is_time_record:\n",
        "                track_end = time_synchronized()\n",
        "                track_time += track_end - infer_end\n",
        "            \n",
        "            if cur_iter == len(self.dataloader) - 1:\n",
        "                result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id]))\n",
        "                write_results_no_score(result_filename, results)\n",
        "\n",
        "        statistics = torch.cuda.FloatTensor([inference_time, track_time, n_samples])\n",
        "        if distributed:\n",
        "            data_list = gather(data_list, dst=0)\n",
        "            data_list = list(itertools.chain(*data_list))\n",
        "            torch.distributed.reduce(statistics, dst=0)\n",
        "\n",
        "        eval_results = self.evaluate_prediction(data_list, statistics)\n",
        "        synchronize()\n",
        "        return eval_results\n",
        "\n",
        "    def evaluate_deepsort(\n",
        "        self,\n",
        "        model,\n",
        "        distributed=False,\n",
        "        half=False,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        test_size=None,\n",
        "        result_folder=None,\n",
        "        model_folder=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        COCO average precision (AP) Evaluation. Iterate inference on the test dataset\n",
        "        and the results are evaluated by COCO API.\n",
        "\n",
        "        NOTE: This function will change training mode to False, please save states if needed.\n",
        "\n",
        "        Args:\n",
        "            model : model to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            ap50_95 (float) : COCO AP of IoU=50:95\n",
        "            ap50 (float) : COCO AP of IoU=50\n",
        "            summary (sr): summary info of evaluation.\n",
        "        \"\"\"\n",
        "        # TODO half to amp_test\n",
        "        tensor_type = torch.cuda.HalfTensor if half else torch.cuda.FloatTensor\n",
        "        model = model.eval()\n",
        "        if half:\n",
        "            model = model.half()\n",
        "        ids = []\n",
        "        data_list = []\n",
        "        results = []\n",
        "        video_names = defaultdict()\n",
        "        progress_bar = tqdm if is_main_process() else iter\n",
        "\n",
        "        inference_time = 0\n",
        "        track_time = 0\n",
        "        n_samples = len(self.dataloader) - 1\n",
        "\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones(1, 3, test_size[0], test_size[1]).cuda()\n",
        "            model(x)\n",
        "            model = model_trt\n",
        "            \n",
        "        tracker = DeepSort(model_folder, min_confidence=self.args.track_thresh)\n",
        "        \n",
        "        for cur_iter, (imgs, _, info_imgs, ids) in enumerate(\n",
        "            progress_bar(self.dataloader)\n",
        "        ):\n",
        "            with torch.no_grad():\n",
        "                # init tracker\n",
        "                frame_id = info_imgs[2].item()\n",
        "                video_id = info_imgs[3].item()\n",
        "                img_file_name = info_imgs[4]\n",
        "                video_name = img_file_name[0].split('/')[0]\n",
        "\n",
        "                if video_name not in video_names:\n",
        "                    video_names[video_id] = video_name\n",
        "                if frame_id == 1:\n",
        "                    tracker = DeepSort(model_folder, min_confidence=self.args.track_thresh)\n",
        "                    if len(results) != 0:\n",
        "                        result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id - 1]))\n",
        "                        write_results_no_score(result_filename, results)\n",
        "                        results = []\n",
        "\n",
        "                imgs = imgs.type(tensor_type)\n",
        "\n",
        "                # skip the the last iters since batchsize might be not enough for batch inference\n",
        "                is_time_record = cur_iter < len(self.dataloader) - 1\n",
        "                if is_time_record:\n",
        "                    start = time.time()\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                if decoder is not None:\n",
        "                    outputs = decoder(outputs, dtype=outputs.type())\n",
        "\n",
        "                outputs = postprocess(outputs, self.num_classes, self.confthre, self.nmsthre)\n",
        "            \n",
        "                if is_time_record:\n",
        "                    infer_end = time_synchronized()\n",
        "                    inference_time += infer_end - start\n",
        "\n",
        "            output_results = self.convert_to_coco_format(outputs, info_imgs, ids)\n",
        "            data_list.extend(output_results)\n",
        "\n",
        "            # run tracking\n",
        "            online_targets = tracker.update(outputs[0], info_imgs, self.img_size, img_file_name[0])\n",
        "            online_tlwhs = []\n",
        "            online_ids = []\n",
        "            for t in online_targets:\n",
        "                tlwh = [t[0], t[1], t[2] - t[0], t[3] - t[1]]\n",
        "                tid = t[4]\n",
        "                vertical = tlwh[2] / tlwh[3] > 1.6\n",
        "                if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:\n",
        "                    online_tlwhs.append(tlwh)\n",
        "                    online_ids.append(tid)\n",
        "            # save results\n",
        "            results.append((frame_id, online_tlwhs, online_ids))\n",
        "\n",
        "            if is_time_record:\n",
        "                track_end = time_synchronized()\n",
        "                track_time += track_end - infer_end\n",
        "            \n",
        "            if cur_iter == len(self.dataloader) - 1:\n",
        "                result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id]))\n",
        "                write_results_no_score(result_filename, results)\n",
        "\n",
        "        statistics = torch.cuda.FloatTensor([inference_time, track_time, n_samples])\n",
        "        if distributed:\n",
        "            data_list = gather(data_list, dst=0)\n",
        "            data_list = list(itertools.chain(*data_list))\n",
        "            torch.distributed.reduce(statistics, dst=0)\n",
        "\n",
        "        eval_results = self.evaluate_prediction(data_list, statistics)\n",
        "        synchronize()\n",
        "        return eval_results\n",
        "\n",
        "    def evaluate_motdt(\n",
        "        self,\n",
        "        model,\n",
        "        distributed=False,\n",
        "        half=False,\n",
        "        trt_file=None,\n",
        "        decoder=None,\n",
        "        test_size=None,\n",
        "        result_folder=None,\n",
        "        model_folder=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        COCO average precision (AP) Evaluation. Iterate inference on the test dataset\n",
        "        and the results are evaluated by COCO API.\n",
        "\n",
        "        NOTE: This function will change training mode to False, please save states if needed.\n",
        "\n",
        "        Args:\n",
        "            model : model to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            ap50_95 (float) : COCO AP of IoU=50:95\n",
        "            ap50 (float) : COCO AP of IoU=50\n",
        "            summary (sr): summary info of evaluation.\n",
        "        \"\"\"\n",
        "        # TODO half to amp_test\n",
        "        tensor_type = torch.cuda.HalfTensor if half else torch.cuda.FloatTensor\n",
        "        model = model.eval()\n",
        "        if half:\n",
        "            model = model.half()\n",
        "        ids = []\n",
        "        data_list = []\n",
        "        results = []\n",
        "        video_names = defaultdict()\n",
        "        progress_bar = tqdm if is_main_process() else iter\n",
        "\n",
        "        inference_time = 0\n",
        "        track_time = 0\n",
        "        n_samples = len(self.dataloader) - 1\n",
        "\n",
        "        if trt_file is not None:\n",
        "            from torch2trt import TRTModule\n",
        "\n",
        "            model_trt = TRTModule()\n",
        "            model_trt.load_state_dict(torch.load(trt_file))\n",
        "\n",
        "            x = torch.ones(1, 3, test_size[0], test_size[1]).cuda()\n",
        "            model(x)\n",
        "            model = model_trt\n",
        "            \n",
        "        tracker = OnlineTracker(model_folder, min_cls_score=self.args.track_thresh)\n",
        "        for cur_iter, (imgs, _, info_imgs, ids) in enumerate(\n",
        "            progress_bar(self.dataloader)\n",
        "        ):\n",
        "            with torch.no_grad():\n",
        "                # init tracker\n",
        "                frame_id = info_imgs[2].item()\n",
        "                video_id = info_imgs[3].item()\n",
        "                img_file_name = info_imgs[4]\n",
        "                video_name = img_file_name[0].split('/')[0]\n",
        "\n",
        "                if video_name not in video_names:\n",
        "                    video_names[video_id] = video_name\n",
        "                if frame_id == 1:\n",
        "                    tracker = OnlineTracker(model_folder, min_cls_score=self.args.track_thresh)\n",
        "                    if len(results) != 0:\n",
        "                        result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id - 1]))\n",
        "                        write_results(result_filename, results)\n",
        "                        results = []\n",
        "\n",
        "                imgs = imgs.type(tensor_type)\n",
        "\n",
        "                # skip the the last iters since batchsize might be not enough for batch inference\n",
        "                is_time_record = cur_iter < len(self.dataloader) - 1\n",
        "                if is_time_record:\n",
        "                    start = time.time()\n",
        "\n",
        "                outputs = model(imgs)\n",
        "                if decoder is not None:\n",
        "                    outputs = decoder(outputs, dtype=outputs.type())\n",
        "\n",
        "                outputs = postprocess(outputs, self.num_classes, self.confthre, self.nmsthre)\n",
        "            \n",
        "                if is_time_record:\n",
        "                    infer_end = time_synchronized()\n",
        "                    inference_time += infer_end - start\n",
        "\n",
        "            output_results = self.convert_to_coco_format(outputs, info_imgs, ids)\n",
        "            data_list.extend(output_results)\n",
        "\n",
        "            # run tracking\n",
        "            online_targets = tracker.update(outputs[0], info_imgs, self.img_size, img_file_name[0])\n",
        "            online_tlwhs = []\n",
        "            online_ids = []\n",
        "            online_scores = []\n",
        "            for t in online_targets:\n",
        "                tlwh = t.tlwh\n",
        "                tid = t.track_id\n",
        "                vertical = tlwh[2] / tlwh[3] > 1.6\n",
        "                if tlwh[2] * tlwh[3] > self.args.min_box_area and not vertical:\n",
        "                    online_tlwhs.append(tlwh)\n",
        "                    online_ids.append(tid)\n",
        "                    online_scores.append(t.score)\n",
        "            # save results\n",
        "            results.append((frame_id, online_tlwhs, online_ids, online_scores))\n",
        "\n",
        "            if is_time_record:\n",
        "                track_end = time_synchronized()\n",
        "                track_time += track_end - infer_end\n",
        "            \n",
        "            if cur_iter == len(self.dataloader) - 1:\n",
        "                result_filename = os.path.join(result_folder, '{}.txt'.format(video_names[video_id]))\n",
        "                write_results(result_filename, results)\n",
        "\n",
        "        statistics = torch.cuda.FloatTensor([inference_time, track_time, n_samples])\n",
        "        if distributed:\n",
        "            data_list = gather(data_list, dst=0)\n",
        "            data_list = list(itertools.chain(*data_list))\n",
        "            torch.distributed.reduce(statistics, dst=0)\n",
        "\n",
        "        eval_results = self.evaluate_prediction(data_list, statistics)\n",
        "        synchronize()\n",
        "        return eval_results\n",
        "\n",
        "    def convert_to_coco_format(self, outputs, info_imgs, ids):\n",
        "        data_list = []\n",
        "        for (output, img_h, img_w, img_id) in zip(\n",
        "            outputs, info_imgs[0], info_imgs[1], ids\n",
        "        ):\n",
        "            if output is None:\n",
        "                continue\n",
        "            output = output.cpu()\n",
        "\n",
        "            bboxes = output[:, 0:4]\n",
        "\n",
        "            # preprocessing: resize\n",
        "            scale = min(\n",
        "                self.img_size[0] / float(img_h), self.img_size[1] / float(img_w)\n",
        "            )\n",
        "            bboxes /= scale\n",
        "            bboxes = xyxy2xywh(bboxes)\n",
        "\n",
        "            cls = output[:, 6]\n",
        "            scores = output[:, 4] * output[:, 5]\n",
        "            for ind in range(bboxes.shape[0]):\n",
        "                label = self.dataloader.dataset.class_ids[int(cls[ind])]\n",
        "                pred_data = {\n",
        "                    \"image_id\": int(img_id),\n",
        "                    \"category_id\": label,\n",
        "                    \"bbox\": bboxes[ind].numpy().tolist(),\n",
        "                    \"score\": scores[ind].numpy().item(),\n",
        "                    \"segmentation\": [],\n",
        "                }  # COCO json format\n",
        "                data_list.append(pred_data)\n",
        "        return data_list\n",
        "\n",
        "    def evaluate_prediction(self, data_dict, statistics):\n",
        "        if not is_main_process():\n",
        "            return 0, 0, None\n",
        "\n",
        "        logger.info(\"Evaluate in main process...\")\n",
        "\n",
        "        annType = [\"segm\", \"bbox\", \"keypoints\"]\n",
        "\n",
        "        inference_time = statistics[0].item()\n",
        "        track_time = statistics[1].item()\n",
        "        n_samples = statistics[2].item()\n",
        "\n",
        "        a_infer_time = 1000 * inference_time / (n_samples * self.dataloader.batch_size)\n",
        "        a_track_time = 1000 * track_time / (n_samples * self.dataloader.batch_size)\n",
        "\n",
        "        time_info = \", \".join(\n",
        "            [\n",
        "                \"Average {} time: {:.2f} ms\".format(k, v)\n",
        "                for k, v in zip(\n",
        "                    [\"forward\", \"track\", \"inference\"],\n",
        "                    [a_infer_time, a_track_time, (a_infer_time + a_track_time)],\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        info = time_info + \"\\n\"\n",
        "\n",
        "        # Evaluate the Dt (detection) json comparing with the ground truth\n",
        "        if len(data_dict) > 0:\n",
        "            cocoGt = self.dataloader.dataset.coco\n",
        "            # TODO: since pycocotools can't process dict in py36, write data to json file.\n",
        "            _, tmp = tempfile.mkstemp()\n",
        "            json.dump(data_dict, open(tmp, \"w\"))\n",
        "            cocoDt = cocoGt.loadRes(tmp)\n",
        "            '''\n",
        "            try:\n",
        "                from yolox.layers import COCOeval_opt as COCOeval\n",
        "            except ImportError:\n",
        "                from pycocotools import cocoeval as COCOeval\n",
        "                logger.warning(\"Use standard COCOeval.\")\n",
        "            '''\n",
        "            #from pycocotools.cocoeval import COCOeval\n",
        "            from yolox.layers import COCOeval_opt as COCOeval\n",
        "            cocoEval = COCOeval(cocoGt, cocoDt, annType[1])\n",
        "            cocoEval.evaluate()\n",
        "            cocoEval.accumulate()\n",
        "            redirect_string = io.StringIO()\n",
        "            with contextlib.redirect_stdout(redirect_string):\n",
        "                cocoEval.summarize()\n",
        "            info += redirect_string.getvalue()\n",
        "            return cocoEval.stats[0], cocoEval.stats[1], info\n",
        "        else:\n",
        "            return 0, 0, info\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n6yHSFti08Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ZUeo2Snp6gaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path(\"/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/pretrained\").mkdir(parents=True, exist_ok=True)\n",
        "!wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth \"/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/pretrained/\""
      ],
      "metadata": {
        "id": "V6qVudxLFno9",
        "outputId": "85eb5e53-9e58-488c-dbaf-f596dd030ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-27 15:28:11--  https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/77a2128d-8fad-4181-a754-0daf70511100?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220927T152811Z&X-Amz-Expires=300&X-Amz-Signature=8c2d8f01c7ec09b8529be103fba2f9e4e914aedaca7b8f5f9a3db14ef0354659&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-27 15:28:11--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/77a2128d-8fad-4181-a754-0daf70511100?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220927T152811Z&X-Amz-Expires=300&X-Amz-Signature=8c2d8f01c7ec09b8529be103fba2f9e4e914aedaca7b8f5f9a3db14ef0354659&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 793388371 (757M) [application/octet-stream]\n",
            "Saving to: ‘yolox_x.pth.1’\n",
            "\n",
            "yolox_x.pth.1       100%[===================>] 756.63M  7.48MB/s    in 47s     \n",
            "\n",
            "2022-09-27 15:28:58 (16.2 MB/s) - ‘yolox_x.pth.1’ saved [793388371/793388371]\n",
            "\n",
            "/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/pretrained/: Scheme missing.\n",
            "FINISHED --2022-09-27 15:28:58--\n",
            "Total wall clock time: 47s\n",
            "Downloaded: 1 files, 757M in 47s (16.2 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/train.py -f '/content/ByteTrack/exps/example/mot/visem.py' -d 1 -b 4 --fp16 -o -c pretrained/yolox_x.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AFkOvF47w9e",
        "outputId": "9d5d3c0d-31f0-4c8a-bd88-f3a41c2e1d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"tools/train.py\", line 112, in <module>\n",
            "    assert num_gpu <= torch.cuda.device_count()\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "5Eh8WLr9kpDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection"
      ],
      "metadata": {
        "id": "m1WhXyDaU1uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Args:\n",
        "    def __init__(self, track_thresh=0.6, track_buffer=30, mot20=False, match_thresh=0.8):\n",
        "        self.track_thresh = track_thresh\n",
        "        self.track_buffer = track_buffer\n",
        "        self.mot20 = mot20\n",
        "        self.match_thresh = match_thresh\n",
        "        self.min_box_area = 10\n",
        "args = Args()\n",
        "model_path = '/content/drive/MyDrive/MediaEval2022_Medico/Trained_models/yolov5_trained_models/weights/best.pt'\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path) #, device='cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAB-54344hkh",
        "outputId": "5d6ec4a5-88fe-48c4-d4f8-341b199a93aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2022-10-5 Python-3.7.14 torch-1.8.2+cu111 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/torch-1.12.1+cu113.dist-info/METADATA'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model summary: 290 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/\n",
        "from exps.example.mot.visem import Exp as Exp\n",
        "exp = Exp()\n",
        "dataloader = exp.get_eval_loader(batch_size=1, is_distributed=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjm_vnei8uzr",
        "outputId": "c498afb6-59f1-43e2-e3db-c12d528a71a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack\n",
            "loading annotations into memory...\n",
            "Done (t=3.81s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader)"
      ],
      "metadata": {
        "id": "JXkP2ssv6zSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfc32e1-dade-45df-e00a-54696ff00b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5880"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import sys\n",
        "spec = importlib.util.spec_from_file_location(\"MOTEvaluator\", \"/content/drive/MyDrive/MediaEval2022_Medico/ByteTrack/yolox/evaluators/mot_evaluator.py\")\n",
        "mot_evaluator = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"MOTEvaluator\"] = mot_evaluator\n",
        "spec.loader.exec_module(mot_evaluator)\n",
        "evaluator = mot_evaluator.MOTEvaluator(dataloader=dataloader, img_size=[640, 640], confthre=0.1, nmsthre=0.7, num_classes=3, args=args)"
      ],
      "metadata": {
        "id": "Qf82tBmQOM2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluator.evaluate(model, result_folder='/content/drive/MyDrive/MediaEval2022_Medico/result/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9l7Uy9xQHvA",
        "outputId": "8b40a6bf-2d0f-4549-cb1f-a3873e7e6a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5880 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            " 25%|██▍       | 1469/5880 [01:32<04:53, 15.03it/s]2022-10-05 11:03:32.412 | INFO     | MOTEvaluator:write_results:40 - save results to /content/drive/MyDrive/MediaEval2022_Medico/result/11.txt\n",
            " 50%|█████     | 2940/5880 [02:58<02:42, 18.11it/s]2022-10-05 11:04:57.528 | INFO     | MOTEvaluator:write_results:40 - save results to /content/drive/MyDrive/MediaEval2022_Medico/result/12.txt\n",
            " 75%|███████▌  | 4410/5880 [04:15<01:15, 19.46it/s]2022-10-05 11:06:13.272 | INFO     | MOTEvaluator:write_results:40 - save results to /content/drive/MyDrive/MediaEval2022_Medico/result/47.txt\n",
            "100%|█████████▉| 5879/5880 [05:41<00:00, 23.34it/s]2022-10-05 11:07:40.499 | INFO     | MOTEvaluator:write_results:40 - save results to /content/drive/MyDrive/MediaEval2022_Medico/result/54.txt\n",
            "100%|██████████| 5880/5880 [05:42<00:00, 17.15it/s]\n",
            "2022-10-05 11:07:40.684 | INFO     | MOTEvaluator:evaluate_prediction:634 - Evaluate in main process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=1.49s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "COCOeval_opt.evaluate() finished in 2.93 seconds.\n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished in 0.42 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Detection result\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vouTlarsuhiI",
        "outputId": "f6ef0da3-3ed5-4ad5-955c-238b6c6c55c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0761638455850668,\n",
              " 0.2174730635272084,\n",
              " 'Average forward time: 43.42 ms, Average track time: 10.18 ms, Average inference time: 53.60 ms\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.217\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.032\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.076\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.039\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.070\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.124\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking "
      ],
      "metadata": {
        "id": "FJccOs2c2Voq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ccwh2xyxy(img_h, img_w, bb_box):\n",
        "    # print(np.array(bbox).shape)\n",
        "    norm_center_x = bb_box[:, 0]\n",
        "    norm_center_y = bb_box[:, 1]\n",
        "    norm_label_width = bb_box[:, 2]\n",
        "    norm_label_height = bb_box[:, 3]\n",
        "    \n",
        "    center_x = norm_center_x * img_w\n",
        "    center_y = norm_center_y * img_h\n",
        "    label_width = norm_label_width * img_w\n",
        "    label_height = norm_label_height * img_h\n",
        "    \n",
        "    x_min = center_x - (label_width/2)\n",
        "    y_min = center_y - (label_height/2)\n",
        "    x_max = center_x + (label_width/2)\n",
        "    y_max = center_y + (label_height/2)\n",
        "    \n",
        "    return np.array([x_min, y_min, x_max, y_max]).T\n",
        "\n",
        "def xyxy2xywh(bboxes):\n",
        "    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n",
        "    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n",
        "    return bboxes"
      ],
      "metadata": {
        "id": "qFDpFvW9czcd"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "val_path = '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/'\n",
        "output_path = '/content/drive/MyDrive/MediaEval2022_Medico/result/gt/'\n",
        "# path = '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/11/labels_ftid/'\n",
        "from zlib import crc32\n",
        "def bytes_to_float(b):\n",
        "    return int(crc32(b) & 0xffffffff) #/ 2**32\n",
        "def str_to_float(s, encoding=\"utf-8\"):\n",
        "    return bytes_to_float(s.encode(encoding))\n",
        "\n",
        "height = 480\n",
        "width = 640\n",
        "\n",
        "for folder in natsorted(glob.glob(os.path.join(val_path, '*'))):\n",
        "    print(folder)\n",
        "    df = pd.DataFrame()\n",
        "    frame_id = 1\n",
        "    for f in natsorted(glob.glob(os.path.join(folder, 'labels_ftid', '*.txt'))):\n",
        "        print(f)\n",
        "        temp = pd.read_csv(f, names=['track_id', 'class', 'x', 'y', 'w', 'h'], sep=' ')\n",
        "        bbox = temp.iloc[:, 2:].to_numpy()\n",
        "        bbox = ccwh2xyxy(height, width, bbox)\n",
        "        bbox = xyxy2xywh(bbox)\n",
        "        temp.iloc[:, 2:] = bbox\n",
        "        temp['frame_id'] = frame_id\n",
        "        temp['track_id'] = temp['track_id'].map(str_to_float)\n",
        "        df = pd.concat([df, temp], axis=0)\n",
        "        frame_id = frame_id + 1\n",
        "        # break\n",
        "    df['conf'] = 1\n",
        "    df['a'] = -1\n",
        "    df['b'] = -1\n",
        "    df['c'] = -1\n",
        "    df = df[['frame_id', 'track_id', 'x', 'y', 'w', 'h', 'conf', 'a', 'b', 'c']]\n",
        "    df = df.sort_values(['track_id', 'frame_id'], ascending=True)\n",
        "    df.to_csv(os.path.join(output_path, '{}.txt'.format(folder.split('/')[-1])), index=False, sep=',', header=False)"
      ],
      "metadata": {
        "id": "n0dNeyaWU9m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import motmetrics as mm\n",
        "from loguru import logger\n",
        "\n",
        "results_folder = '/content/drive/MyDrive/MediaEval2022_Medico/result/pred/'\n",
        "\n",
        "def compare_dataframes(gts, ts):\n",
        "    accs = []\n",
        "    names = []\n",
        "    for k, tsacc in ts.items():\n",
        "        if k in gts:            \n",
        "            logger.info('Comparing {}...'.format(k))\n",
        "            accs.append(mm.utils.compare_to_groundtruth(gts[k], tsacc, 'iou', distth=0.5))\n",
        "            names.append(k)\n",
        "        else:\n",
        "            logger.warning('No ground truth for {}, skipping.'.format(k))\n",
        "\n",
        "    return accs, names\n",
        "\n",
        "mm.lap.default_solver = 'lap'\n",
        "\n",
        "# if exp.val_ann == 'val_half.json':\n",
        "#     gt_type = '_val_half'\n",
        "# else:\n",
        "#     gt_type = ''\n",
        "# print('gt_type', gt_type)\n",
        "# if args.mot20:\n",
        "#     gtfiles = glob.glob(os.path.join('datasets/MOT20/train', '*/gt/gt{}.txt'.format(gt_type)))\n",
        "# else:\n",
        "gtfiles = glob.glob(os.path.join('/content/drive/MyDrive/MediaEval2022_Medico/result/gt/', '*.txt'))\n",
        "print('gt_files', gtfiles)\n",
        "tsfiles = [f for f in glob.glob(os.path.join(results_folder, '*.txt')) if not os.path.basename(f).startswith('eval')]\n",
        "\n",
        "logger.info('Found {} groundtruths and {} test files.'.format(len(gtfiles), len(tsfiles)))\n",
        "logger.info('Available LAP solvers {}'.format(mm.lap.available_solvers))\n",
        "logger.info('Default LAP solver \\'{}\\''.format(mm.lap.default_solver))\n",
        "logger.info('Loading files.')\n",
        "\n",
        "gt = OrderedDict([(os.path.splitext(Path(f).parts[-1])[0], mm.io.loadtxt(f, fmt='mot15-2D', min_confidence=1)) for f in gtfiles])\n",
        "ts = OrderedDict([(os.path.splitext(Path(f).parts[-1])[0], mm.io.loadtxt(f, fmt='mot15-2D', min_confidence=-1)) for f in tsfiles])    \n",
        "\n",
        "mh = mm.metrics.create()    \n",
        "accs, names = compare_dataframes(gt, ts)\n",
        "\n",
        "logger.info('Running metrics')\n",
        "metrics = ['recall', 'precision', 'num_unique_objects', 'mostly_tracked',\n",
        "            'partially_tracked', 'mostly_lost', 'num_false_positives', 'num_misses',\n",
        "            'num_switches', 'num_fragmentations', 'mota', 'motp', 'num_objects']\n",
        "summary = mh.compute_many(accs, names=names, metrics=metrics, generate_overall=True)\n",
        "# summary = mh.compute_many(accs, names=names, metrics=mm.metrics.motchallenge_metrics, generate_overall=True)\n",
        "# print(mm.io.render_summary(\n",
        "#   summary, formatters=mh.formatters, \n",
        "#   namemap=mm.io.motchallenge_metric_names))\n",
        "div_dict = {\n",
        "    'num_objects': ['num_false_positives', 'num_misses', 'num_switches', 'num_fragmentations'],\n",
        "    'num_unique_objects': ['mostly_tracked', 'partially_tracked', 'mostly_lost']}\n",
        "for divisor in div_dict:\n",
        "    for divided in div_dict[divisor]:\n",
        "        summary[divided] = (summary[divided] / summary[divisor])\n",
        "fmt = mh.formatters\n",
        "change_fmt_list = ['num_false_positives', 'num_misses', 'num_switches', 'num_fragmentations', 'mostly_tracked',\n",
        "                    'partially_tracked', 'mostly_lost']\n",
        "for k in change_fmt_list:\n",
        "    fmt[k] = fmt['mota']\n",
        "print(mm.io.render_summary(summary, formatters=fmt, namemap=mm.io.motchallenge_metric_names))\n",
        "\n",
        "metrics = mm.metrics.motchallenge_metrics + ['num_objects']\n",
        "summary = mh.compute_many(accs, names=names, metrics=metrics, generate_overall=True)\n",
        "print(mm.io.render_summary(summary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names))\n",
        "logger.info('Completed')"
      ],
      "metadata": {
        "id": "AZy4_40R2XZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f5d7a2-e3c6-455c-84ae-76a0f04f0579"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-06 06:51:18.459 | INFO     | __main__:<module>:34 - Found 4 groundtruths and 4 test files.\n",
            "2022-10-06 06:51:18.461 | INFO     | __main__:<module>:35 - Available LAP solvers ['lap', 'scipy']\n",
            "2022-10-06 06:51:18.467 | INFO     | __main__:<module>:36 - Default LAP solver 'lap'\n",
            "2022-10-06 06:51:18.470 | INFO     | __main__:<module>:37 - Loading files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt_files ['/content/drive/MyDrive/MediaEval2022_Medico/result/gt/11.txt', '/content/drive/MyDrive/MediaEval2022_Medico/result/gt/12.txt', '/content/drive/MyDrive/MediaEval2022_Medico/result/gt/47.txt', '/content/drive/MyDrive/MediaEval2022_Medico/result/gt/54.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-06 06:51:23.034 | INFO     | __main__:compare_dataframes:12 - Comparing 11...\n",
            "2022-10-06 06:51:25.994 | INFO     | __main__:compare_dataframes:12 - Comparing 12...\n",
            "2022-10-06 06:51:28.262 | INFO     | __main__:compare_dataframes:12 - Comparing 47...\n",
            "2022-10-06 06:51:29.567 | INFO     | __main__:compare_dataframes:12 - Comparing 54...\n",
            "2022-10-06 06:51:31.941 | INFO     | __main__:<module>:45 - Running metrics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Rcll  Prcn  GT    MT    PT    ML    FP    FN  IDs   FM   MOTA  MOTP num_objects\n",
            "11      81.6% 71.2%  57 61.4% 35.1%  3.5% 33.0% 18.4% 0.1% 1.1%  48.6% 0.307       56568\n",
            "12      71.7% 78.3% 117 35.0% 59.0%  6.0% 19.9% 28.3% 0.7% 4.0%  51.2% 0.312       39357\n",
            "47      75.6% 57.6%  24 41.7% 45.8% 12.5% 55.7% 24.4% 0.1% 2.7%  19.8% 0.326        8035\n",
            "54      15.2% 14.5%  79  2.5% 16.5% 81.0% 89.2% 84.8% 0.0% 1.1% -74.0% 0.419       42263\n",
            "OVERALL 59.4% 55.9% 277 31.8% 40.8% 27.4% 46.9% 40.6% 0.2% 2.0%  12.2% 0.318      146223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-06 06:51:33.999 | INFO     | __main__:<module>:70 - Completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         IDF1   IDP   IDR  Rcll  Prcn  GT MT  PT ML    FP    FN IDs    FM   MOTA  MOTP IDt IDa IDm num_objects\n",
            "11      68.7% 64.4% 73.8% 81.6% 71.2%  57 35  20  2 18650 10397  50   623  48.6% 0.307  13  33   0       56568\n",
            "12      54.9% 57.4% 52.6% 71.7% 78.3% 117 41  69  7  7838 11121 258  1592  51.2% 0.312  42 203   3       39357\n",
            "47      63.9% 56.3% 73.9% 75.6% 57.6%  24 10  11  3  4477  1960  11   215  19.8% 0.326   1  10   0        8035\n",
            "54      13.3% 13.0% 13.5% 15.2% 14.5%  79  2  13 64 37683 35859   7   454 -74.0% 0.419   5   4   2       42263\n",
            "OVERALL 49.1% 47.6% 50.7% 59.4% 55.9% 277 88 113 76 68648 59337 326  2884  12.2% 0.318  61 250   5      146223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "LNimrOWdQw9H",
        "outputId": "ed318995-1aa0-47eb-9f2c-8104d7511227"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             idf1       idp       idr    recall  precision  \\\n",
              "11       0.687410  0.643649  0.737555  0.816204   0.712285   \n",
              "12       0.548581  0.573543  0.525701  0.717433   0.782724   \n",
              "47       0.638834  0.562642  0.738892  0.756067   0.575720   \n",
              "54       0.132554  0.129812  0.135414  0.151527   0.145258   \n",
              "OVERALL  0.490938  0.476243  0.506569  0.594202   0.558630   \n",
              "\n",
              "         num_unique_objects  mostly_tracked  partially_tracked  mostly_lost  \\\n",
              "11                       57              35                 20            2   \n",
              "12                      117              41                 69            7   \n",
              "47                       24              10                 11            3   \n",
              "54                       79               2                 13           64   \n",
              "OVERALL                 277              88                113           76   \n",
              "\n",
              "         num_false_positives  num_misses  num_switches  num_fragmentations  \\\n",
              "11                     18650       10397            50                 623   \n",
              "12                      7838       11121           258                1592   \n",
              "47                      4477        1960            11                 215   \n",
              "54                     37683       35859             7                 454   \n",
              "OVERALL                68648       59337           326                2884   \n",
              "\n",
              "             mota      motp  num_transfer  num_ascend  num_migrate  \\\n",
              "11       0.485628  0.306983            13          33            0   \n",
              "12       0.511726  0.311962            42         203            3   \n",
              "47       0.197511  0.325955             1          10            0   \n",
              "54      -0.740269  0.419143             5           4            2   \n",
              "OVERALL  0.122498  0.318194            61         250            5   \n",
              "\n",
              "         num_objects  \n",
              "11             56568  \n",
              "12             39357  \n",
              "47              8035  \n",
              "54             42263  \n",
              "OVERALL       146223  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90614518-6e7e-4b9f-b81c-7ac84a558498\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idf1</th>\n",
              "      <th>idp</th>\n",
              "      <th>idr</th>\n",
              "      <th>recall</th>\n",
              "      <th>precision</th>\n",
              "      <th>num_unique_objects</th>\n",
              "      <th>mostly_tracked</th>\n",
              "      <th>partially_tracked</th>\n",
              "      <th>mostly_lost</th>\n",
              "      <th>num_false_positives</th>\n",
              "      <th>num_misses</th>\n",
              "      <th>num_switches</th>\n",
              "      <th>num_fragmentations</th>\n",
              "      <th>mota</th>\n",
              "      <th>motp</th>\n",
              "      <th>num_transfer</th>\n",
              "      <th>num_ascend</th>\n",
              "      <th>num_migrate</th>\n",
              "      <th>num_objects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.687410</td>\n",
              "      <td>0.643649</td>\n",
              "      <td>0.737555</td>\n",
              "      <td>0.816204</td>\n",
              "      <td>0.712285</td>\n",
              "      <td>57</td>\n",
              "      <td>35</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>18650</td>\n",
              "      <td>10397</td>\n",
              "      <td>50</td>\n",
              "      <td>623</td>\n",
              "      <td>0.485628</td>\n",
              "      <td>0.306983</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>56568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.548581</td>\n",
              "      <td>0.573543</td>\n",
              "      <td>0.525701</td>\n",
              "      <td>0.717433</td>\n",
              "      <td>0.782724</td>\n",
              "      <td>117</td>\n",
              "      <td>41</td>\n",
              "      <td>69</td>\n",
              "      <td>7</td>\n",
              "      <td>7838</td>\n",
              "      <td>11121</td>\n",
              "      <td>258</td>\n",
              "      <td>1592</td>\n",
              "      <td>0.511726</td>\n",
              "      <td>0.311962</td>\n",
              "      <td>42</td>\n",
              "      <td>203</td>\n",
              "      <td>3</td>\n",
              "      <td>39357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.638834</td>\n",
              "      <td>0.562642</td>\n",
              "      <td>0.738892</td>\n",
              "      <td>0.756067</td>\n",
              "      <td>0.575720</td>\n",
              "      <td>24</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>4477</td>\n",
              "      <td>1960</td>\n",
              "      <td>11</td>\n",
              "      <td>215</td>\n",
              "      <td>0.197511</td>\n",
              "      <td>0.325955</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>8035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.132554</td>\n",
              "      <td>0.129812</td>\n",
              "      <td>0.135414</td>\n",
              "      <td>0.151527</td>\n",
              "      <td>0.145258</td>\n",
              "      <td>79</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>64</td>\n",
              "      <td>37683</td>\n",
              "      <td>35859</td>\n",
              "      <td>7</td>\n",
              "      <td>454</td>\n",
              "      <td>-0.740269</td>\n",
              "      <td>0.419143</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>42263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OVERALL</th>\n",
              "      <td>0.490938</td>\n",
              "      <td>0.476243</td>\n",
              "      <td>0.506569</td>\n",
              "      <td>0.594202</td>\n",
              "      <td>0.558630</td>\n",
              "      <td>277</td>\n",
              "      <td>88</td>\n",
              "      <td>113</td>\n",
              "      <td>76</td>\n",
              "      <td>68648</td>\n",
              "      <td>59337</td>\n",
              "      <td>326</td>\n",
              "      <td>2884</td>\n",
              "      <td>0.122498</td>\n",
              "      <td>0.318194</td>\n",
              "      <td>61</td>\n",
              "      <td>250</td>\n",
              "      <td>5</td>\n",
              "      <td>146223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90614518-6e7e-4b9f-b81c-7ac84a558498')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90614518-6e7e-4b9f-b81c-7ac84a558498 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90614518-6e7e-4b9f-b81c-7ac84a558498');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate using Coco API a single image"
      ],
      "metadata": {
        "id": "brVqwdEIj-IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from yolox.utils.boxes import xyxy2xywh, xyxy2cxcywh\n",
        "from copy import deepcopy\n",
        "img_size=[640, 640]\n",
        "def convert_to_coco_format(outputs, info_imgs, ids):\n",
        "        data_list = []\n",
        "        for (output, img_h, img_w, img_id) in zip(\n",
        "            outputs, info_imgs[0], info_imgs[1], ids\n",
        "        ):\n",
        "            if output is None:\n",
        "                continue\n",
        "            output = output.cpu()\n",
        "            bboxes = deepcopy(output[:, 0:4])\n",
        "            # print(bboxes.shape)\n",
        "\n",
        "            # preprocessing: resize\n",
        "            scale = min(\n",
        "                img_size[0] / float(img_h), img_size[1] / float(img_w)\n",
        "            )\n",
        "            bboxes /= scale\n",
        "            bboxes = xyxy2xywh(bboxes)\n",
        "\n",
        "\n",
        "            cls = output[:, 5]\n",
        "            scores = output[:, 4]\n",
        "            for ind in range(bboxes.shape[0]):\n",
        "                # label = dataloader.dataset.class_ids[int(cls[ind])]\n",
        "                pred_data = {\n",
        "                    \"image_id\": int(img_id),\n",
        "                    \"category_id\": int(cls[ind]),#label,\n",
        "                    \"bbox\": bboxes[ind].numpy().tolist(),\n",
        "                    \"score\": scores[ind].numpy().item(),\n",
        "                    \"segmentation\": [],\n",
        "                }  # COCO json format\n",
        "                data_list.append(pred_data)\n",
        "        return data_list"
      ],
      "metadata": {
        "id": "NKLUw3mTSaaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "video_names = defaultdict()\n",
        "ids = []\n",
        "imgs, _, info_imgs, ids = temp\n",
        "frame_id = info_imgs[2].item()\n",
        "video_id = int(info_imgs[3][0])\n",
        "img_file_name = info_imgs[4][0]\n",
        "video_name = img_file_name.split('/')[-1]\n",
        "if video_name not in video_names:\n",
        "    video_names[video_id] = video_name"
      ],
      "metadata": {
        "id": "SdIjjHGTbDEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(info_imgs[-1][0])"
      ],
      "metadata": {
        "id": "sc19euLUblb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_results = convert_to_coco_format(outputs.pred, info_imgs, ids)\n",
        "data_dict = []\n",
        "data_dict.extend(output_results)"
      ],
      "metadata": {
        "id": "5CMNpkPxbH4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import tempfile\n",
        "import io\n",
        "annType = [\"segm\", \"bbox\", \"keypoints\"]\n",
        "if len(data_dict) > 0:\n",
        "    cocoGt = dataloader.dataset.coco\n",
        "    # TODO: since pycocotools can't process dict in py36, write data to json file.\n",
        "    _, tmp = tempfile.mkstemp()\n",
        "    json.dump(data_dict, open(tmp, \"w\"))\n",
        "    cocoDt = cocoGt.loadRes(tmp)\n",
        "    '''\n",
        "    try:\n",
        "        from yolox.layers import COCOeval_opt as COCOeval\n",
        "    except ImportError:\n",
        "        from pycocotools import cocoeval as COCOeval\n",
        "        logger.warning(\"Use standard COCOeval.\")\n",
        "    '''\n",
        "    #from pycocotools.cocoeval import COCOeval\n",
        "    from yolox.layers import COCOeval_opt as COCOeval\n",
        "    cocoEval = COCOeval(cocoGt, cocoDt, annType[1])\n",
        "    cocoEval.evaluate()\n",
        "    cocoEval.accumulate()\n",
        "    redirect_string = io.StringIO()\n",
        "    with contextlib.redirect_stdout(redirect_string):\n",
        "        cocoEval.summarize()\n",
        "    # info += redirect_string.getvalue()\n",
        "    print(cocoEval.stats[0], cocoEval.stats[1])\n",
        "    #, info\n",
        "else:\n",
        "    print(0, 0)\n",
        "    #, info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOExZA3yeW-B",
        "outputId": "32eb220f-c04d-4d3f-bca1-b9fee2d3c457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "COCOeval_opt.evaluate() finished in 2.10 seconds.\n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished in 0.03 seconds.\n",
            "0.0018901890189018903 0.0033003300330033004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/annotations/Val.json') as f:\n",
        "    d = json.load(f)"
      ],
      "metadata": {
        "id": "XsNl-f4Uf_Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/MediaEval2022_Medico/result/12.json', 'w') as f:\n",
        "    json.dump(data_dict, f)"
      ],
      "metadata": {
        "id": "7R45ZHZohQ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "cocoGt=COCO('/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/annotations/Val.json')\n",
        "cocoDt=cocoGt.loadRes('/content/drive/MyDrive/MediaEval2022_Medico/result/12.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0-yLu-Ks8Td",
        "outputId": "47fcb199-1a4c-40b6-f8e6-34a5867b19a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cocoGt.dataset['images'][0])\n",
        "print(cocoDt.dataset['images'][0])\n",
        "print(cocoGt.dataset['annotations'][0])\n",
        "print(cocoDt.dataset['annotations'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNQCrvf3yS9K",
        "outputId": "71ee7b5d-1c49-4913-8992-672612f0d7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_name': '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/11/images/11_frame_0.jpg', 'id': 1, 'frame_id': 1, 'prev_image_id': -1, 'next_image_id': 2, 'video_id': '11', 'height': 480, 'width': 640}\n",
            "{'file_name': '/content/drive/MyDrive/MediaEval2022_Medico/VISEM_Tracking_Train_v4/Val/11/images/11_frame_0.jpg', 'id': 1, 'frame_id': 1, 'prev_image_id': -1, 'next_image_id': 2, 'video_id': '11', 'height': 480, 'width': 640}\n",
            "{'id': 1, 'category_id': 0, 'image_id': 1, 'track_id': 1, 'bbox': [167.99115044247787, 189.023598820059, 17.0, 18.0], 'conf': 1, 'iscrowd': 0, 'area': 0.00099609375}\n",
            "{'image_id': 1, 'category_id': 0, 'bbox': [164.95285034179688, 189.93438720703125, 18.43402099609375, 16.795867919921875], 'score': 0.8593279123306274, 'segmentation': [], 'area': 309.6153818834573, 'id': 1, 'iscrowd': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(cocoDt.dataset['annotations'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knkhsQL59aCS",
        "outputId": "d4a475de-e98a-4d33-dd37-b865dbbfdbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgIds=sorted(cocoGt.getImgIds(imgIds=[1]))\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,annType[1])\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1JTXqlghdo9",
        "outputId": "81b6c149-d60e-47cb-b1b3-c174c1e7539a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.00s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.367\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.907\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.081\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.367\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
          ]
        }
      ]
    }
  ]
}